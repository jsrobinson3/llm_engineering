{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba1305e-c763-4b82-b94d-b36e6a6cf11a",
   "metadata": {},
   "source": [
    "# ðŸ§  Multi-LLM Brochure Generator (GPT, Claude, LLaMA)\n",
    "Generate a company brochure using OpenAI, Anthropic, and locally hosted LLaMA 3.2 models â€” all streaming side-by-side in real time using Gradio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bb818-28fc-45ec-9851-9b40da068fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai google-generativeai anthropic ollama gradio requests beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0b5b7-e23c-4507-80db-e5ac06138fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "import threading\n",
    "from queue import Queue\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai\n",
    "import ollama\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API setup\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "claude = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "google.generativeai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "system_message = \"You are a helpful AI assistant that generates compelling company brochures.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf11cc1-54a8-4083-8f67-5bd509c02dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            tag.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e065cc9-4d57-4036-b64c-22853c3682f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt, queue):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        if content:\n",
    "            queue.put((\"GPT\", content))\n",
    "    queue.put((\"GPT\", None))\n",
    "\n",
    "def stream_claude(prompt, queue):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-sonnet-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            if text:\n",
    "                queue.put((\"Claude\", text))\n",
    "    queue.put((\"Claude\", None))\n",
    "\n",
    "def stream_llama(prompt, queue):\n",
    "    stream = ollama.chat(\n",
    "        model=\"llama3:latest\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        if content:\n",
    "            queue.put((\"LLaMA\", content))\n",
    "    queue.put((\"LLaMA\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92351704-f8ae-48c8-a4ca-7f463f1a4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt, queue):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        if content:\n",
    "            queue.put((\"GPT\", content))\n",
    "    queue.put((\"GPT\", None))\n",
    "\n",
    "def stream_claude(prompt, queue):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            if text:\n",
    "                queue.put((\"Claude\", text))\n",
    "    queue.put((\"Claude\", None))\n",
    "\n",
    "def stream_llama(prompt, queue):\n",
    "    try:\n",
    "        stream = ollama.chat(\n",
    "            model=\"llama3.2\",  # or your preferred tag\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            content_piece = chunk['message']['content']\n",
    "            if content_piece:\n",
    "                queue.put((\"LLaMA\", content_piece))\n",
    "    except Exception as e:\n",
    "        queue.put((\"LLaMA\", f\"\\n[Error using Ollama client: {str(e)}]\\n\"))\n",
    "    finally:\n",
    "        queue.put((\"LLaMA\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2a3a7-2a68-4833-a4db-4ca1b2fd110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_worker(company_name, url, model, queue):\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt += Website(url).get_contents()\n",
    "    if model == \"GPT\":\n",
    "        stream_gpt(prompt, queue)\n",
    "    elif model == \"Claude\":\n",
    "        stream_claude(prompt, queue)\n",
    "    elif model == \"LLaMA\":\n",
    "        stream_llama(prompt, queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9b835-48d4-4680-aa92-90ac264a8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochures(company_name, url, selected_models):\n",
    "    output = {\"GPT\": \"\", \"Claude\": \"\", \"LLaMA\": \"\"}\n",
    "    queue = Queue()\n",
    "    threads = []\n",
    "\n",
    "    for model in selected_models:\n",
    "        t = threading.Thread(target=stream_worker, args=(company_name, url, model, queue))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    finished_models = set()\n",
    "    while len(finished_models) < len(selected_models):\n",
    "        model, chunk = queue.get()\n",
    "        if chunk is None:\n",
    "            finished_models.add(model)\n",
    "            continue\n",
    "        output[model] += chunk\n",
    "        yield output.get(\"GPT\", \"\"), output.get(\"Claude\", \"\"), output.get(\"LLaMA\", \"\")\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411bc14-2107-4104-83f5-0a770904a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        company_input = gr.Textbox(label=\"Company name:\")\n",
    "        url_input = gr.Textbox(label=\"Landing page URL\")\n",
    "        model_input = gr.CheckboxGroup([\"GPT\", \"Claude\", \"LLaMA\"], label=\"Select model(s)\")\n",
    "\n",
    "    run_btn = gr.Button(\"Generate Brochures\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### GPT Brochure\")\n",
    "            output_gpt = gr.Markdown()\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Claude Brochure\")\n",
    "            output_claude = gr.Markdown()\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### LLaMA Brochure\")\n",
    "            output_llama = gr.Markdown()\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=stream_brochures,\n",
    "        inputs=[company_input, url_input, model_input],\n",
    "        outputs=[output_gpt, output_claude, output_llama]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b5e54-b93c-451b-ab68-b8094ce3f183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
